{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import typing\n",
    "import io\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "from PIL import Image\n",
    "#from torchvision import transforms\n",
    "from models.transformers import VisionTransformer, CONFIGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (transformer): Transformer(\n",
       "    (embeddings): Embeddings(\n",
       "      (patch_embeddings): Conv2d(5, 320, kernel_size=(8, 8), stride=(8, 8))\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "    (encoder): Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): Block(\n",
       "          (attention_norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=320, out_features=512, bias=True)\n",
       "            (fc2): Linear(in_features=512, out_features=320, bias=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (key): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (value): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (out): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (1): Block(\n",
       "          (attention_norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=320, out_features=512, bias=True)\n",
       "            (fc2): Linear(in_features=512, out_features=320, bias=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (key): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (value): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (out): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (2): Block(\n",
       "          (attention_norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=320, out_features=512, bias=True)\n",
       "            (fc2): Linear(in_features=512, out_features=320, bias=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (key): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (value): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (out): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (3): Block(\n",
       "          (attention_norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=320, out_features=512, bias=True)\n",
       "            (fc2): Linear(in_features=512, out_features=320, bias=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (key): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (value): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (out): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (encoder_norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (head): Linear(in_features=320, out_features=2, bias=True)\n",
       "  (softmax): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_type = 'ViT-L_32'\n",
    "config = CONFIGS[model_type]\n",
    "model = VisionTransformer(config, img_size=32, num_classes=2,vis = True)\n",
    "model.load_state_dict(torch.load('/data1/fog/TransMAP/EXPs/EX004_2D_FiveTop_Patch/best_model_EX004_2D_FiveTop_Patch.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import dataloader\n",
    "import json\n",
    "\n",
    "DEFAULT_IMAGE_DIR_NAME    = '/data1/fog-data/fog-maps/'\n",
    "DEFAULT_TARGET_DIR_NAME   = '/data1/fog/Dataset/TARGET'\n",
    "year_information          = {'2009':['20090101', '20091231'],\n",
    "                            '2010':['20100101', '20101231'],\n",
    "                            '2011':['20110101', '20111231'],\n",
    "                            '2012':['20120101', '20121231'],\n",
    "                            '2013':['20130101', '20131231'],\n",
    "                            '2014':['20140101', '20141231'],\n",
    "                            '2015':['20150101', '20151231'],\n",
    "                            '2016':['20160101', '20161231'],\n",
    "                            '2017':['20170101', '20171231'],\n",
    "                            '2018':['20180101', '20181231'],\n",
    "                            '2019':['20190101', '20191231'],\n",
    "                            '2020':['20200101', '20201215']}\n",
    "\n",
    "#data_split_dict = \n",
    "            \n",
    "dataset = dataloader.input_dataframe_generater(img_path = None, \n",
    "                                            target_path = None, \n",
    "                                            first_date_string = year_information['2009'][0], \n",
    "                                            last_date_string  = year_information['2020'][1], \n",
    "                                            target_binarizing_thre = 1).dataframe_generation()\n",
    "\n",
    "# split the data into train, validation and test:\n",
    "train_df, valid_df, test_df = dataloader.split_data_train_valid_test(dataset, year_split_dict = {'train': ['2013', '2014', '2015', '2016', '2017'], 'valid': ['2009', '2010', '2011'], 'test': ['2018', '2019', '2020']})\n",
    "with open('/data1/fog/TransMAP/EXPs/EX002_2D_FiveTop/mean_std_EX002_2D_FiveTop.json', 'r') as file:\n",
    "    norm_mean_std_dict = json.load(file)\n",
    "\n",
    "\n",
    "test_dataset = dataloader.DataAdopter(test_df, \n",
    "                                        map_structure         = '2D', \n",
    "                                        predictor_names       = dataloader.NETCDF_PREDICTOR_NAMES['Five_Top'], \n",
    "                                        lead_time_pred        = 24, \n",
    "                                        mean_std_dict         = norm_mean_std_dict,\n",
    "                                        point_geolocation_dic = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = test_dataset[1977]['input']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 32, 32])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 4)\n"
     ]
    }
   ],
   "source": [
    "logits, att_mat = model(x.unsqueeze(0))\n",
    "att_mat = torch.stack(att_mat).squeeze(1)\n",
    "# Average the attention weights across all heads.\n",
    "att_mat = torch.mean(att_mat, dim=1)\n",
    "# To account for residual connections, we add an identity matrix to the\n",
    "# attention matrix and re-normalize the weights.\n",
    "residual_att = torch.eye(att_mat.size(1))\n",
    "aug_att_mat = att_mat + residual_att\n",
    "aug_att_mat = aug_att_mat / aug_att_mat.sum(dim=-1).unsqueeze(-1)\n",
    "\n",
    "# Recursively multiply the weight matrices\n",
    "joint_attentions = torch.zeros(aug_att_mat.size())\n",
    "joint_attentions[0] = aug_att_mat[0]\n",
    "\n",
    "for n in range(1, aug_att_mat.size(0)):\n",
    "    joint_attentions[n] = torch.matmul(aug_att_mat[n], joint_attentions[n-1])\n",
    "    \n",
    "# Attention from the output token to the input space.\n",
    "v = joint_attentions[-1]\n",
    "grid_size = int(np.sqrt(aug_att_mat.size(-1)))\n",
    "mask = v[0, 1:].reshape(grid_size, grid_size).detach().numpy()\n",
    "\n",
    "print(mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = cv2.resize((mask / mask.max()), (32, 32))[..., np.newaxis]\n",
    "result = (mask * x.detach().numpy()[0, :, :]).astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 ('deep-learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e69a14dfbdec224c5e932b370c8d5491158516262a04bb50e9804f010a1e34c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
